{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38164bitddd67c07f803415ab1baa72a5157e93e",
   "display_name": "Python 3.8.1 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import re\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = thai_stopwords()\n",
    "def clean_tag(text):\n",
    "  text = re.sub('<.*?>','',text)\n",
    "  return text\n",
    "def complete_clean(text :str):\n",
    "  \n",
    "  # Table for emoticon\n",
    "  emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            u\"\\U0001f926-\\U0001f937\"\n",
    "                            u\"\\U00010000-\\U0010ffff\"\n",
    "                            u\"\\u2640-\\u2642\"\n",
    "                            u\"\\u2600-\\u2B55\"\n",
    "                            u\"\\u200d\"\n",
    "                            u\"\\u23cf\"\n",
    "                            u\"\\u23e9\"\n",
    "                            u\"\\u231a\"\n",
    "                            u\"\\ufe0f\"  # dingbats\n",
    "                            u\"\\u3030\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "\n",
    "  # clean paunsuation\n",
    "  nopunc_text = re.sub(r\"[·“”\\\"\\\\,@\\'?\\$%_\\[\\]()/*+<=>!`~{|}^:;,-.&#]\", \"\", text, flags=re.I)\n",
    "\n",
    "  # clean emoticon\n",
    "  clean_text = emoji_pattern.sub(r'', nopunc_text)\n",
    "\n",
    "  # clean tag html\n",
    "  cleantext_text = clean_tag(clean_text)\n",
    "\n",
    "  return cleantext_text\n",
    "def notdetermine(x):\n",
    "  return x not in stopword\n",
    "def removeStopword (arr):\n",
    "  somelist = [x for x in arr if notdetermine(x)]\n",
    "  return somelist\n",
    "def tokenization(text):\n",
    "  # token the sentence\n",
    "  token_text = word_tokenize(text, engine=\"newmm\", keep_whitespace=False)\n",
    "  return token_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'../data/DataTank.json') as f:\n",
    "  alldata = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data preprocess finish!!!!!!\n"
     ]
    }
   ],
   "source": [
    "dataPreProcessings = {}\n",
    "for kratoo in alldata:\n",
    "  kratooContent = alldata[kratoo]\n",
    "\n",
    "  # preprocessing\n",
    "  titleDes = kratooContent['title']+kratooContent['desc']\n",
    "    # clean process\n",
    "  CleanTxt = complete_clean(titleDes)\n",
    "  # data token \n",
    "  dataToken = tokenization(CleanTxt)\n",
    "  # remove stop word\n",
    "  bareText = removeStopword(dataToken)\n",
    "\n",
    "  dataPreProcessing  = kratooContent\n",
    "  dataPreProcessing['title'] = bareText\n",
    "  dataPreProcessing['desc'] = ''\n",
    "\n",
    "  dataPreProcessings[dataPreProcessing['tid']] = dataPreProcessing\n",
    "\n",
    "# save datapreprocessing\n",
    "with open(f\"../data/dataPreProcess.json\",mode='w') as datasave:\n",
    "  content = json.dump(dataPreProcessings, datasave, ensure_ascii=False)\n",
    "\n",
    "print('Data preprocess finish!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 179 ms, sys: 83.1 ms, total: 263 ms\nWall time: 308 ms\nTFIDF finish!!!!!!\n"
     ]
    }
   ],
   "source": [
    "############# TF-idf #############\n",
    "\n",
    "listData = []\n",
    "for kratoo in dataPreProcessings:\n",
    "  listData.append(dataPreProcessings[kratoo]['title'])\n",
    "\n",
    "tokens_listData = [','.join(tkn) for tkn in listData]\n",
    "tfidf = TfidfVectorizer(analyzer=lambda x:x.split(','),)\n",
    "try:\n",
    "  %time tfidf_resutl = tfidf.fit_transform(tokens_listData).todense()\n",
    "except:\n",
    "  print('cant do tf-idf')\n",
    "\n",
    "print('TFIDF finish!!!!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'DBSCAN' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e139bcff01e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m############# DBscan #############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdbscan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_resutl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfinalResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DBSCAN' is not defined"
     ]
    }
   ],
   "source": [
    "############# DBscan #############\n",
    "\n",
    "dbscan = DBSCAN(eps=50, min_samples=3)\n",
    "db = dbscan.fit(tfidf_resutl) \n",
    "finalResult = (db.labels_).tolist()\n",
    "print(len(finalResult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}